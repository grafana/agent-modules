/*
Module: job-node-exporter
Description: Scrapes Node Exporter, this is a separate scrape job, if you are also using annotation based scraping, you will want to explicitly
             disable node-exporter from being scraped by this module and annotations by setting the following annotation on the node-exporter
             metrics.agent.grafana.com/scrape: "false".

             Node exporter should be deployed as a DaemonSet, each pod on each worker will be scraped by the agent using endpoint service discovery

Note: Every argument except for "forward_to" is optional, and does have a defined default value.  However, the values for these
      arguments are not defined using the default = " ... " argument syntax, but rather using the coalesce(argument.value, " ... ").
      This is because if the argument passed in from another consuming module is set to null, the default = " ... " syntax will
      does not override the value passed in, where coalesce() will return the first non-null value.
*/
argument "forward_to" {
  comment = "Must be a list(MetricsReceiver) where collected logs should be forwarded to"
  optional = false
}

argument "enabled" {
  comment = "Whether or not the node-exporter job should be enabled, this is useful for disabling the job when it is being consumed by other modules in a multi-tenancy environment"
  optional = true
}

argument "namespaces" {
  comment = "The namespaces to look for targets in (default: [] is all namespaces)"
  optional = true
}

argument "selectors" {
  // see: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
  comment = "The label selectors to use to find matching targets (default: [\"app.kubernetes.io/name=prometheus-node-exporter\"])"
  optional = true
}

argument "port_name" {
  comment = "The of the port to scrape metrics from"
  optional = true
}

argument "job_label" {
  comment = "The job label to add for all node-exporter metrics (default: integrations/node_exporter)"
  optional = true
}

argument "keep_metrics" {
  comment = "A regex of metrics to keep (default: see below)"
  optional = true
}

argument "scrape_interval" {
  comment = "How often to scrape metrics from the targets (default: 60s)"
  optional = true
}

argument "scrape_timeout" {
  comment = "How long before a scrape times out (default: 10s)"
  optional = true
}

argument "max_cache_size" {
  comment = "The maximum number of elements to hold in the relabeling cache (default: 100000).  Only increase if the amount of metrics returned is extremely large, the default will almost always be sufficient"
  optional = true
}

argument "clustering" {
  // Docs: https://grafana.com/docs/agent/latest/flow/concepts/clustering/
  comment = "Whether or not clustering should be enabled (default: false)"
  optional = true
}

// node exporter service discovery for all of the pods in the node exporter daemonset
discovery.kubernetes "node_exporter" {
  role = "pod"

  selectors {
    role = "pod"
    label = join(coalesce(argument.selectors.value, ["app.kubernetes.io/name=prometheus-node-exporter"]), ",")
  }

  namespaces {
    names = coalesce(argument.namespaces.value, [])
  }
}

// node exporter relabelings (pre-scrape)
discovery.relabel "node_exporter" {
  targets = discovery.kubernetes.node_exporter.targets

  // drop all targets if enabled is false
  rule {
    target_label = "__enabled"
    replacement = format("%s", coalesce(argument.enabled.value, "true"))
  }
  rule {
    source_labels = ["__enabled"]
    regex = "false"
    action = "drop"
  }

  // keep only the specified metrics port name, and pods that are Running and ready
  rule {
    source_labels = [
      "__meta_kubernetes_pod_container_port_name",
      "__meta_kubernetes_pod_phase",
      "__meta_kubernetes_pod_ready",
    ]
    separator = "@"
    regex = coalesce(argument.port_name.value, "metrics") + "@Running@true"
    action = "keep"
  }

  // copy the pod name to the instance label
  rule {
    source_labels = ["__meta_kubernetes_pod_node_name"]
    action = "replace"
    target_label = "instance"
  }
}

// node exporter scrape job
prometheus.scrape "node_exporter" {
  job_name = coalesce(argument.job_label.value, "integrations/node_exporter")
  forward_to = [prometheus.relabel.node_exporter.receiver]
  targets = discovery.relabel.node_exporter.output
  scrape_interval = coalesce(argument.scrape_interval.value, "60s")
  scrape_timeout = coalesce(argument.scrape_timeout.value, "10s")

  clustering {
    enabled = coalesce(argument.clustering.value, false)
  }
}

// node-exporter metric relabelings (post-scrape)
prometheus.relabel "node_exporter" {
  forward_to = argument.forward_to.value
  max_cache_size = coalesce(argument.max_cache_size.value, 100000)

  // keep only metrics that match the keep_metrics regex
  rule {
    source_labels = ["__name__"]
    regex = coalesce(argument.keep_metrics.value, "(up|node_cpu.*|node_exporter_build_info|node_filesystem.*|node_memory.*|process_cpu_seconds_total|process_resident_memory_bytes)")
    action = "keep"
  }

  // Drop metrics for certain file systems
  rule {
    source_labels = ["__name__", "fstype"]
    separator = "@"
    regex = "node_filesystem.*@(tempfs)"
    action = "drop"
  }
}
